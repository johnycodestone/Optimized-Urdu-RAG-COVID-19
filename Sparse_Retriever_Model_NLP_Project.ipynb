{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO/zfTlnS7TADIKtHii7KFk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnycodestone/Optimized-Urdu-RAG-COVID-19/blob/main/Sparse_Retriever_Model_NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dI6A-aXsWFP3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba78188-3ea1-4f54-eb6e-26cd9161edd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install required libraries (run this cell first and one by one all required libraries will be installed)\n",
        "# - transformers: model + generation\n",
        "# - sentence-transformers: dense embeddings / fine-tuning helpers\n",
        "# - faiss-cpu (or faiss-gpu if GPU available)\n",
        "# - rank_bm25: BM25 baseline\n",
        "# - datasets: convenient JSONL loading\n",
        "# - evaluate / sacrebleu: BLEU/chrF metrics\n",
        "# - tqdm: progress bars\n",
        "# - accelerate (optional) for distributed/faster training\n",
        "!pip install -q transformers sentence-transformers faiss-cpu rank_bm25 datasets evaluate sacrebleu tqdm accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to google drive if not already connected\n",
        "# 2. Mount Google Drive\n",
        "# We need this to load your fine-tuned Dense Retriever and your Corpus file.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xStS7FjvWTHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef5ad4e0-bb52-4313-a394-02c5e651cc55"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional cell\n",
        "# To add all the required files run\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "EP3FrehEWjWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list # Optional to run this cell: To check which of the libraries/packages have been installed"
      ],
      "metadata": {
        "id": "cUXINKDnWrOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and GPU check: Run this cell after the first cell\n",
        "import os, json, time, math\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Transformers / sentence-transformers\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
        "import sentence_transformers # Import the package itself to access __version__\n",
        "\n",
        "# FAISS and BM25\n",
        "import faiss\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Datasets and metrics\n",
        "from datasets import load_dataset, Dataset\n",
        "import evaluate\n",
        "import sacrebleu\n",
        "\n",
        "# Print versions and GPU info\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"sentence-transformers:\", sentence_transformers.__version__)\n",
        "try:\n",
        "    import torch\n",
        "    print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
        "except Exception as e:\n",
        "    print(\"torch not available:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROVrf4DxWtkM",
        "outputId": "9733eb97-a0aa-4364-a426-d9174320a861"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers: 4.57.3\n",
            "sentence-transformers: 5.2.0\n",
            "torch: 2.9.0+cu126 cuda: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load JSONL/TSV files into Python structures\n",
        "# There will be a content folder on left side bar, files panel. This is our root\n",
        "# folder. Inside it create a data folder, if not already present. Upload all files\n",
        "# there and then run this cell.\n",
        "\n",
        "DATA_DIR = Path(\"drive/MyDrive/data\")  # change if files are elsewhere\n",
        "\n",
        "# Create the data directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def load_jsonl(path):\n",
        "    items = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                items.append(json.loads(line))\n",
        "    return items\n",
        "\n",
        "corpus_clean = load_jsonl(DATA_DIR / \"urdu_covid_corpus_clean.jsonl\")\n",
        "passages_min = load_jsonl(DATA_DIR / \"urdu_covid_passages_min.jsonl\")\n",
        "# TSV -> list of dicts\n",
        "passages_tsv = []\n",
        "with open(DATA_DIR / \"urdu_covid_passages.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Use split(None, 1) to split on the first occurrence of any whitespace\n",
        "        # This handles cases where the delimiter might be spaces instead of a tab.\n",
        "        if line.strip(): # Ensure line is not empty after stripping whitespace\n",
        "            parts = line.rstrip(\"\\n\").split(None, 1)\n",
        "            if len(parts) == 2:\n",
        "                pid, text = parts\n",
        "                passages_tsv.append({\"id\": pid, \"text\": text})\n",
        "            else:\n",
        "                print(f\"Skipping malformed line in urdu_covid_passages.tsv: {line.strip()}\")\n",
        "\n",
        "eval_queries = load_jsonl(DATA_DIR / \"eval_queries.jsonl\")\n",
        "synthetic_pairs = load_jsonl(DATA_DIR / \"synthetic_qa_pairs.jsonl\")\n",
        "hard_negatives = load_jsonl(DATA_DIR / \"hard_negatives.jsonl\")\n",
        "\n",
        "print(\"Loaded:\", len(corpus_clean), \"corpus_clean; \", len(passages_min), \"passages_min; \", len(eval_queries), \"eval queries\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JEeJKevWwDv",
        "outputId": "d2438eff-3021-4eb9-bb4f-090958cdc8e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: 60 corpus_clean;  60 passages_min;  100 eval queries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Validate IDs referenced in eval/synthetic/hard_negatives exist in corpus\n",
        "# Run this after Cell 3.\n",
        "passage_ids = {p[\"id\"] for p in passages_min}\n",
        "missing = []\n",
        "for q in eval_queries:\n",
        "    for pid in q.get(\"positive_ids\", []):\n",
        "        if pid not in passage_ids:\n",
        "            missing.append((\"eval\", q[\"query_id\"], pid))\n",
        "for s in synthetic_pairs:\n",
        "    if s[\"positive_id\"] not in passage_ids:\n",
        "        missing.append((\"synthetic\", s[\"synthetic_id\"], s[\"positive_id\"]))\n",
        "for h in hard_negatives:\n",
        "    for pid in h[\"hard_negatives\"]:\n",
        "        if pid not in passage_ids:\n",
        "            missing.append((\"hardneg\", h[\"query_id\"], pid))\n",
        "print(\"Missing references (should be zero):\", len(missing))\n",
        "if missing:\n",
        "    print(missing[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHTlURX9W2QR",
        "outputId": "3a438f7a-da1f-4444-d3ea-24cfd7840bba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing references (should be zero): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 (Run after Cell 4): BM25 baseline index (tokenize with simple whitespace; for Urdu this is OK as baseline)\n",
        "# We'll store tokenized corpus and BM25 object for retrieval.\n",
        "from nltk.tokenize import word_tokenize\n",
        "# If nltk not installed, use simple split\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab') # Added to resolve LookupError for 'punkt_tab'\n",
        "    tokenizer = lambda s: word_tokenize(s)\n",
        "except Exception:\n",
        "    tokenizer = lambda s: s.split()\n",
        "\n",
        "corpus_texts = [p[\"text\"] for p in passages_min]\n",
        "corpus_ids = [p[\"id\"] for p in passages_min]\n",
        "tokenized_corpus = [tokenizer(t) for t in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# Example retrieval function\n",
        "def bm25_retrieve(query, k=5):\n",
        "    q_tokens = tokenizer(query)\n",
        "    scores = bm25.get_scores(q_tokens)\n",
        "    topk = np.argsort(scores)[::-1][:k]\n",
        "    return [(corpus_ids[i], corpus_texts[i], float(scores[i])) for i in topk]\n",
        "\n",
        "# Quick test\n",
        "print(\"BM25 top-3 for sample:\", bm25_retrieve(eval_queries[0][\"query\"], k=3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "robOLEwmW4X_",
        "outputId": "37480d48-c913-4697-80f6-1d6c615ec84c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 top-3 for sample: [('p0001', 'کورونا وائرس مرض 2019 (COVID-19) ایک متعدی بیماری ہے جس کی عام علامات میں بخار، کھانسی اور سانس لینے میں دشواری شامل ہیں۔', 5.810063974702894), ('p0024', 'بچوں میں کووڈ-19 عام طور پر ہلکا ہوتا ہے مگر بعض نادر معاملات میں شدید علامات سامنے آ سکتی ہیں؛ بچوں کے لیے مخصوص رہنمائی مختلف ہو سکتی ہے۔', 5.103496839739362), ('p0002', 'کووڈ-19 کی تشخیص کے لیے rRT-PCR سویب ٹیسٹ عام طور پر استعمال ہوتے ہیں اور یہ وائرس کی موجودگی کی تصدیق کرتے ہیں۔', 4.589270107579207)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5b: BM25-only retriever evaluation tool (run after Cell 5)\n",
        "# Purpose: standalone evaluation harness for the independent BM25 retriever (bm25_retrieve)\n",
        "# Metrics included (applicable to a retriever-only evaluation):\n",
        "#   - Recall@1, Recall@5\n",
        "#   - MRR (Mean Reciprocal Rank)\n",
        "#   - Precision@k (k=1,5)\n",
        "#   - Average / median retrieval latency\n",
        "#   - Optional: match by gold_passage_id or by substring match of gold_answer\n",
        "# Output:\n",
        "#   - Per-query JSONL saved to bm25_eval_results.jsonl\n",
        "#   - Printed summary with all metrics\n",
        "#\n",
        "# Requirements (must be available in the session):\n",
        "#   - bm25_retrieve(query, k) -> list of (passage_id, passage_text, score)\n",
        "#   - eval_queries: list of dicts with at least a query field and optionally:\n",
        "#       * \"question\" or \"query\" or \"q\"  (the query text)\n",
        "#       * \"gold_passage_id\" (optional) OR \"answer\"/\"gold\" (gold text to match)\n",
        "#\n",
        "# Usage:\n",
        "#   - Run this cell after you build the BM25 index (Cell 5).\n",
        "#   - Optionally pass a different eval list or k values to evaluate subsets.\n",
        "\n",
        "# Use this evaluator if your eval_queries items contain \"positive_ids\" and \"gold_answer\"\n",
        "import json, time, re, statistics\n",
        "from typing import List, Dict\n",
        "\n",
        "OUT_JSONL = \"bm25_eval_results.jsonl\"\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    if s is None: return \"\"\n",
        "    s = str(s).strip()\n",
        "    return re.sub(r\"\\s+\", \" \", s)\n",
        "\n",
        "def get_query_text(item: Dict) -> str:\n",
        "    return item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "\n",
        "def evaluate_bm25_with_positive_ids(eval_items: List[Dict],\n",
        "                                    out_jsonl: str = OUT_JSONL,\n",
        "                                    k: int = DEFAULT_K,\n",
        "                                    recall_ks = RECALL_KS,\n",
        "                                    precision_ks = PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies = []\n",
        "    rr_list = []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in eval_items:\n",
        "        total += 1\n",
        "        q = get_query_text(item)\n",
        "        positive_ids = item.get(\"positive_ids\") or item.get(\"positive_id\") or []\n",
        "        # normalize to list of strings\n",
        "        if isinstance(positive_ids, str):\n",
        "            positive_ids = [positive_ids]\n",
        "        positive_ids = [str(x) for x in positive_ids]\n",
        "\n",
        "        gold_text = normalize_text(item.get(\"gold_answer\") or item.get(\"answer\") or \"\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            hits = bm25_retrieve(q, k=k)   # (id, text, score)\n",
        "        except Exception as e:\n",
        "            hits = []\n",
        "            print(f\"[eval] bm25_retrieve error for query {q[:60]}... -> {e}\")\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [h[0] for h in hits]\n",
        "        retrieved_texts = [h[1] for h in hits]\n",
        "\n",
        "        # Reciprocal rank: first position among positives\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in positive_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k (multiple positives supported)\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in positive_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            # precision@k = (# positives in top-k) / k\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in positive_ids)\n",
        "            precision_sums[pk] += (num_pos_in_topk / pk)\n",
        "\n",
        "        per_query.append({\n",
        "            \"query_id\": item.get(\"query_id\"),\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": positive_ids,\n",
        "            \"gold_text\": gold_text,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"retrieved_texts_preview\": [t[:300] for t in retrieved_texts],\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    mrr = sum(rr_list) / n\n",
        "    recall_at = {rk: recall_counts[rk] / n for rk in recall_ks}\n",
        "    precision_at = {pk: precision_sums[pk] / n for pk in precision_ks}\n",
        "    latency_mean = statistics.mean(latencies) if latencies else 0.0\n",
        "    latency_median = statistics.median(latencies) if latencies else 0.0\n",
        "\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": mrr,\n",
        "        **{f\"Recall@{rk}\": recall_at[rk] for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_at[pk] for pk in precision_ks},\n",
        "        \"latency_mean_s\": latency_mean,\n",
        "        \"latency_median_s\": latency_median\n",
        "    }\n",
        "\n",
        "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in per_query:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# Run it\n",
        "if 'eval_queries' not in globals():\n",
        "    # try to load from file if not in memory\n",
        "    eval_queries = []\n",
        "    with open(\"eval_queries.jsonl\",\"r\",encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            eval_queries.append(json.loads(line))\n",
        "\n",
        "summary, records = evaluate_bm25_with_positive_ids(eval_queries, out_jsonl=OUT_JSONL, k=DEFAULT_K)\n",
        "print(\"BM25 retrieval evaluation summary:\")\n",
        "for k,v in summary.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# show a few examples where retrieval missed positives\n",
        "misses = [r for r in records if r[\"reciprocal_rank\"] == 0.0]\n",
        "print(f\"\\nTotal misses: {len(misses)} / {len(records)}. Showing up to 5 misses:\")\n",
        "for r in misses[:5]:\n",
        "    print(\"Query id:\", r.get(\"query_id\"), \"Query:\", r[\"query\"][:80])\n",
        "    print(\" Positives:\", r[\"positive_ids\"])\n",
        "    print(\" Retrieved top ids:\", r[\"retrieved_ids\"][:8])\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlQQTQZhW-hG",
        "outputId": "e56b74ae-5491-494e-b384-eefa024f09a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 retrieval evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.8853333333333333\n",
            "  Recall@1: 0.84\n",
            "  Recall@5: 0.95\n",
            "  Precision@1: 0.84\n",
            "  Precision@5: 0.21599999999999964\n",
            "  latency_mean_s: 0.00026865243911743163\n",
            "  latency_median_s: 0.0002652406692504883\n",
            "\n",
            "Total misses: 5 / 100. Showing up to 5 misses:\n",
            "Query id: q007 Query: کووڈ-19 ویکسین کا بنیادی مقصد کیا ہے؟\n",
            " Positives: ['p0007']\n",
            " Retrieved top ids: ['p0028', 'p0050', 'p0051', 'p0027', 'p0039']\n",
            "\n",
            "Query id: q019 Query: وینٹیلیشن وبا کے دوران کیوں اہم ہے؟\n",
            " Positives: ['p0020']\n",
            " Retrieved top ids: ['p0017', 'p0060', 'p0031', 'p0048', 'p0027']\n",
            "\n",
            "Query id: q038 Query: ویکسین سائیڈ ایفیکٹس کی نگرانی کیسے کی جاتی ہے؟\n",
            " Positives: ['p0039']\n",
            " Retrieved top ids: ['p0058', 'p0040', 'p0032', 'p0051', 'p0011']\n",
            "\n",
            "Query id: q065 Query: ویکسین کی سائیڈ ایفیکٹس کی رپورٹنگ کیسے ہوتی ہے؟\n",
            " Positives: ['p0039']\n",
            " Retrieved top ids: ['p0058', 'p0047', 'p0032', 'p0022', 'p0025']\n",
            "\n",
            "Query id: q095 Query: وبا کے دوران معاشی بحالی کے لیے کون سے اقدامات کیے جا سکتے ہیں؟\n",
            " Positives: ['p0054']\n",
            " Retrieved top ids: ['p0060', 'p0035', 'p0044', 'p0028', 'p0057']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results/accuracy of the sparse retriever model are exceptional and we are safe to move on to the dense retriever model."
      ],
      "metadata": {
        "id": "aCgql706XGkt"
      }
    }
  ]
}