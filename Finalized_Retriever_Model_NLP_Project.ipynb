{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMFKIBCFhQzQgJllIkXjop7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnycodestone/Optimized-Urdu-RAG-COVID-19/blob/main/Finalized_Retriever_Model_NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cells 1 - 5b: Sparse Retriver Model (B2M5)\n",
        "\n",
        "Cells 6 - 8c: Dense Retriver Model and its fine tuning (FAISS)\n",
        "\n",
        "Cells 9 - 9b: Hybrid/Finalized Retriver Model (Both B2M5 and Dense fused together)"
      ],
      "metadata": {
        "id": "OBATo6ujM0dX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gM7oermDMNQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d486f5-a6d6-479e-c922-831c07401fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install required libraries (run this cell first and one by one all required libraries will be installed)\n",
        "# - transformers: model + generation\n",
        "# - sentence-transformers: dense embeddings / fine-tuning helpers\n",
        "# - faiss-cpu (or faiss-gpu if GPU available)\n",
        "# - rank_bm25: BM25 baseline\n",
        "# - datasets: convenient JSONL loading\n",
        "# - evaluate / sacrebleu: BLEU/chrF metrics\n",
        "# - tqdm: progress bars\n",
        "# - accelerate (optional) for distributed/faster training\n",
        "!pip install -q transformers sentence-transformers faiss-cpu rank_bm25 datasets evaluate sacrebleu tqdm accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to google drive if not already connected\n",
        "# 2. Mount Google Drive\n",
        "# We need this to load your fine-tuned Dense Retriever and your Corpus file.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyjayIxvM30W",
        "outputId": "8a3d374c-bcb1-483d-9dee-a098c2616038"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional cell\n",
        "# To add all the required files run\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "WA7kUbj8M6_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list # Optional to run this cell: To check which of the libraries/packages have been installed"
      ],
      "metadata": {
        "id": "-Wlfayt0NDzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and GPU check: Run this cell after the first cell\n",
        "import os, json, time, math\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Transformers / sentence-transformers\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
        "import sentence_transformers # Import the package itself to access __version__\n",
        "\n",
        "# FAISS and BM25\n",
        "import faiss\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Datasets and metrics\n",
        "from datasets import load_dataset, Dataset\n",
        "import evaluate\n",
        "import sacrebleu\n",
        "\n",
        "# Print versions and GPU info\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"sentence-transformers:\", sentence_transformers.__version__)\n",
        "try:\n",
        "    import torch\n",
        "    print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
        "except Exception as e:\n",
        "    print(\"torch not available:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWDtXiz1NFiY",
        "outputId": "d9e7da0c-60d9-4bca-83e3-59019b441677"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers: 4.57.3\n",
            "sentence-transformers: 5.2.0\n",
            "torch: 2.9.0+cu126 cuda: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load JSONL/TSV files into Python structures\n",
        "# There will be a content folder on left side bar, files panel. This is our root\n",
        "# folder. Inside it create a data folder, if not already present. Upload all files\n",
        "# there and then run this cell.\n",
        "\n",
        "DATA_DIR = Path(\"drive/MyDrive/data\")  # change if files are elsewhere\n",
        "\n",
        "# Create the data directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def load_jsonl(path):\n",
        "    items = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                items.append(json.loads(line))\n",
        "    return items\n",
        "\n",
        "corpus_clean = load_jsonl(DATA_DIR / \"urdu_covid_corpus_clean.jsonl\")\n",
        "passages_min = load_jsonl(DATA_DIR / \"urdu_covid_passages_min.jsonl\")\n",
        "# TSV -> list of dicts\n",
        "passages_tsv = []\n",
        "with open(DATA_DIR / \"urdu_covid_passages.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Use split(None, 1) to split on the first occurrence of any whitespace\n",
        "        # This handles cases where the delimiter might be spaces instead of a tab.\n",
        "        if line.strip(): # Ensure line is not empty after stripping whitespace\n",
        "            parts = line.rstrip(\"\\n\").split(None, 1)\n",
        "            if len(parts) == 2:\n",
        "                pid, text = parts\n",
        "                passages_tsv.append({\"id\": pid, \"text\": text})\n",
        "            else:\n",
        "                print(f\"Skipping malformed line in urdu_covid_passages.tsv: {line.strip()}\")\n",
        "\n",
        "eval_queries = load_jsonl(DATA_DIR / \"eval_queries.jsonl\")\n",
        "synthetic_pairs = load_jsonl(DATA_DIR / \"synthetic_qa_pairs.jsonl\")\n",
        "hard_negatives = load_jsonl(DATA_DIR / \"hard_negatives.jsonl\")\n",
        "\n",
        "print(\"Loaded:\", len(corpus_clean), \"corpus_clean; \", len(passages_min), \"passages_min; \", len(eval_queries), \"eval queries\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppq0RZQ5NG7F",
        "outputId": "648f6003-20db-4911-fb02-cd595b168717"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: 60 corpus_clean;  60 passages_min;  100 eval queries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Validate IDs referenced in eval/synthetic/hard_negatives exist in corpus\n",
        "# Run this after Cell 3.\n",
        "passage_ids = {p[\"id\"] for p in passages_min}\n",
        "missing = []\n",
        "for q in eval_queries:\n",
        "    for pid in q.get(\"positive_ids\", []):\n",
        "        if pid not in passage_ids:\n",
        "            missing.append((\"eval\", q[\"query_id\"], pid))\n",
        "for s in synthetic_pairs:\n",
        "    if s[\"positive_id\"] not in passage_ids:\n",
        "        missing.append((\"synthetic\", s[\"synthetic_id\"], s[\"positive_id\"]))\n",
        "for h in hard_negatives:\n",
        "    for pid in h[\"hard_negatives\"]:\n",
        "        if pid not in passage_ids:\n",
        "            missing.append((\"hardneg\", h[\"query_id\"], pid))\n",
        "print(\"Missing references (should be zero):\", len(missing))\n",
        "if missing:\n",
        "    print(missing[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tODubHnLNMlJ",
        "outputId": "701f0eb1-2050-4c4c-bf7d-4e05b1161c90"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing references (should be zero): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 (Run after Cell 4): BM25 baseline index (tokenize with simple whitespace; for Urdu this is OK as baseline)\n",
        "# We'll store tokenized corpus and BM25 object for retrieval.\n",
        "from nltk.tokenize import word_tokenize\n",
        "# If nltk not installed, use simple split\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab') # Added to resolve LookupError for 'punkt_tab'\n",
        "    tokenizer = lambda s: word_tokenize(s)\n",
        "except Exception:\n",
        "    tokenizer = lambda s: s.split()\n",
        "\n",
        "corpus_texts = [p[\"text\"] for p in passages_min]\n",
        "corpus_ids = [p[\"id\"] for p in passages_min]\n",
        "tokenized_corpus = [tokenizer(t) for t in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# Example retrieval function\n",
        "def bm25_retrieve(query, k=5):\n",
        "    q_tokens = tokenizer(query)\n",
        "    scores = bm25.get_scores(q_tokens)\n",
        "    topk = np.argsort(scores)[::-1][:k]\n",
        "    return [(corpus_ids[i], corpus_texts[i], float(scores[i])) for i in topk]\n",
        "\n",
        "# Quick test\n",
        "print(\"BM25 top-3 for sample:\", bm25_retrieve(eval_queries[0][\"query\"], k=3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a42fMrDnNNKx",
        "outputId": "267d8647-aa04-445b-a6ec-fbce73934bfd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 top-3 for sample: [('p0001', 'کورونا وائرس مرض 2019 (COVID-19) ایک متعدی بیماری ہے جس کی عام علامات میں بخار، کھانسی اور سانس لینے میں دشواری شامل ہیں۔', 5.810063974702894), ('p0024', 'بچوں میں کووڈ-19 عام طور پر ہلکا ہوتا ہے مگر بعض نادر معاملات میں شدید علامات سامنے آ سکتی ہیں؛ بچوں کے لیے مخصوص رہنمائی مختلف ہو سکتی ہے۔', 5.103496839739362), ('p0002', 'کووڈ-19 کی تشخیص کے لیے rRT-PCR سویب ٹیسٹ عام طور پر استعمال ہوتے ہیں اور یہ وائرس کی موجودگی کی تصدیق کرتے ہیں۔', 4.589270107579207)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5b: BM25-only retriever evaluation tool (run after Cell 5)\n",
        "# Purpose: standalone evaluation harness for the independent BM25 retriever (bm25_retrieve)\n",
        "# Metrics included (applicable to a retriever-only evaluation):\n",
        "#   - Recall@1, Recall@5\n",
        "#   - MRR (Mean Reciprocal Rank)\n",
        "#   - Precision@k (k=1,5)\n",
        "#   - Average / median retrieval latency\n",
        "#   - Optional: match by gold_passage_id or by substring match of gold_answer\n",
        "# Output:\n",
        "#   - Per-query JSONL saved to bm25_eval_results.jsonl\n",
        "#   - Printed summary with all metrics\n",
        "#\n",
        "# Requirements (must be available in the session):\n",
        "#   - bm25_retrieve(query, k) -> list of (passage_id, passage_text, score)\n",
        "#   - eval_queries: list of dicts with at least a query field and optionally:\n",
        "#       * \"question\" or \"query\" or \"q\"  (the query text)\n",
        "#       * \"gold_passage_id\" (optional) OR \"answer\"/\"gold\" (gold text to match)\n",
        "#\n",
        "# Usage:\n",
        "#   - Run this cell after you build the BM25 index (Cell 5).\n",
        "#   - Optionally pass a different eval list or k values to evaluate subsets.\n",
        "\n",
        "# Use this evaluator if your eval_queries items contain \"positive_ids\" and \"gold_answer\"\n",
        "import json, time, re, statistics\n",
        "from typing import List, Dict\n",
        "\n",
        "OUT_JSONL = \"bm25_eval_results.jsonl\"\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    if s is None: return \"\"\n",
        "    s = str(s).strip()\n",
        "    return re.sub(r\"\\s+\", \" \", s)\n",
        "\n",
        "def get_query_text(item: Dict) -> str:\n",
        "    return item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "\n",
        "def evaluate_bm25_with_positive_ids(eval_items: List[Dict],\n",
        "                                    out_jsonl: str = OUT_JSONL,\n",
        "                                    k: int = DEFAULT_K,\n",
        "                                    recall_ks = RECALL_KS,\n",
        "                                    precision_ks = PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies = []\n",
        "    rr_list = []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in eval_items:\n",
        "        total += 1\n",
        "        q = get_query_text(item)\n",
        "        positive_ids = item.get(\"positive_ids\") or item.get(\"positive_id\") or []\n",
        "        # normalize to list of strings\n",
        "        if isinstance(positive_ids, str):\n",
        "            positive_ids = [positive_ids]\n",
        "        positive_ids = [str(x) for x in positive_ids]\n",
        "\n",
        "        gold_text = normalize_text(item.get(\"gold_answer\") or item.get(\"answer\") or \"\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            hits = bm25_retrieve(q, k=k)   # (id, text, score)\n",
        "        except Exception as e:\n",
        "            hits = []\n",
        "            print(f\"[eval] bm25_retrieve error for query {q[:60]}... -> {e}\")\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [h[0] for h in hits]\n",
        "        retrieved_texts = [h[1] for h in hits]\n",
        "\n",
        "        # Reciprocal rank: first position among positives\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in positive_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k (multiple positives supported)\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in positive_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            # precision@k = (# positives in top-k) / k\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in positive_ids)\n",
        "            precision_sums[pk] += (num_pos_in_topk / pk)\n",
        "\n",
        "        per_query.append({\n",
        "            \"query_id\": item.get(\"query_id\"),\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": positive_ids,\n",
        "            \"gold_text\": gold_text,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"retrieved_texts_preview\": [t[:300] for t in retrieved_texts],\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    mrr = sum(rr_list) / n\n",
        "    recall_at = {rk: recall_counts[rk] / n for rk in recall_ks}\n",
        "    precision_at = {pk: precision_sums[pk] / n for pk in precision_ks}\n",
        "    latency_mean = statistics.mean(latencies) if latencies else 0.0\n",
        "    latency_median = statistics.median(latencies) if latencies else 0.0\n",
        "\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": mrr,\n",
        "        **{f\"Recall@{rk}\": recall_at[rk] for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_at[pk] for pk in precision_ks},\n",
        "        \"latency_mean_s\": latency_mean,\n",
        "        \"latency_median_s\": latency_median\n",
        "    }\n",
        "\n",
        "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in per_query:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# Run it\n",
        "if 'eval_queries' not in globals():\n",
        "    # try to load from file if not in memory\n",
        "    eval_queries = []\n",
        "    with open(\"eval_queries.jsonl\",\"r\",encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            eval_queries.append(json.loads(line))\n",
        "\n",
        "summary, records = evaluate_bm25_with_positive_ids(eval_queries, out_jsonl=OUT_JSONL, k=DEFAULT_K)\n",
        "print(\"BM25 retrieval evaluation summary:\")\n",
        "for k,v in summary.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# show a few examples where retrieval missed positives\n",
        "misses = [r for r in records if r[\"reciprocal_rank\"] == 0.0]\n",
        "print(f\"\\nTotal misses: {len(misses)} / {len(records)}. Showing up to 5 misses:\")\n",
        "for r in misses[:5]:\n",
        "    print(\"Query id:\", r.get(\"query_id\"), \"Query:\", r[\"query\"][:80])\n",
        "    print(\" Positives:\", r[\"positive_ids\"])\n",
        "    print(\" Retrieved top ids:\", r[\"retrieved_ids\"][:8])\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO9MPuSyNO2X",
        "outputId": "fb233f75-7f2a-4924-9517-108175ced0a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 retrieval evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.8853333333333333\n",
            "  Recall@1: 0.84\n",
            "  Recall@5: 0.95\n",
            "  Precision@1: 0.84\n",
            "  Precision@5: 0.21599999999999964\n",
            "  latency_mean_s: 0.0002596735954284668\n",
            "  latency_median_s: 0.0002568960189819336\n",
            "\n",
            "Total misses: 5 / 100. Showing up to 5 misses:\n",
            "Query id: q007 Query: کووڈ-19 ویکسین کا بنیادی مقصد کیا ہے؟\n",
            " Positives: ['p0007']\n",
            " Retrieved top ids: ['p0028', 'p0050', 'p0051', 'p0027', 'p0039']\n",
            "\n",
            "Query id: q019 Query: وینٹیلیشن وبا کے دوران کیوں اہم ہے؟\n",
            " Positives: ['p0020']\n",
            " Retrieved top ids: ['p0017', 'p0060', 'p0031', 'p0048', 'p0027']\n",
            "\n",
            "Query id: q038 Query: ویکسین سائیڈ ایفیکٹس کی نگرانی کیسے کی جاتی ہے؟\n",
            " Positives: ['p0039']\n",
            " Retrieved top ids: ['p0058', 'p0040', 'p0032', 'p0051', 'p0011']\n",
            "\n",
            "Query id: q065 Query: ویکسین کی سائیڈ ایفیکٹس کی رپورٹنگ کیسے ہوتی ہے؟\n",
            " Positives: ['p0039']\n",
            " Retrieved top ids: ['p0058', 'p0047', 'p0032', 'p0022', 'p0025']\n",
            "\n",
            "Query id: q095 Query: وبا کے دوران معاشی بحالی کے لیے کون سے اقدامات کیے جا سکتے ہیں؟\n",
            " Positives: ['p0054']\n",
            " Retrieved top ids: ['p0060', 'p0035', 'p0044', 'p0028', 'p0057']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Dense embeddings with a multilingual model (use a compact model for Colab)\n",
        "# We use a multilingual SBERT model that supports Urdu reasonably (e.g., 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "embed_model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "embedder = SentenceTransformer(embed_model_name)\n",
        "\n",
        "# Compute embeddings for passages_min (batching)\n",
        "passage_embeddings = embedder.encode(corpus_texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# Build FAISS index (cosine similarity via normalized vectors)\n",
        "d = passage_embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(d)  # inner product\n",
        "# normalize embeddings for cosine\n",
        "faiss.normalize_L2(passage_embeddings)\n",
        "index.add(passage_embeddings)\n",
        "\n",
        "# Map index positions to ids\n",
        "# retrieval function\n",
        "def dense_retrieve(query, k=5):\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    results = []\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "        results.append((corpus_ids[idx], corpus_texts[idx], float(score)))\n",
        "    return results\n",
        "\n",
        "# Quick test\n",
        "print(\"Dense top-3:\", dense_retrieve(eval_queries[0][\"query\"], k=3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406,
          "referenced_widgets": [
            "e585cdd9062c4d6b995405ab3f304f17",
            "4951951e2b4a4b4486168b50d6a65442",
            "01e16f3aa80e4ff08bd159c611bbc3f6",
            "ebe65b7722934328935496478861a5a7",
            "3d558691464c479ead568eb11ea17b6a",
            "0c77821c8a154af0a6094eefe07284c0",
            "91fe671065b245fc95cd119cc87b5b38",
            "636f943dcbfe480ca727f15020fc39d3",
            "e6f3714e6efd40afbb6a6aec78eb734a",
            "9db3ff0784294ac1bb4117b243e54e51",
            "f0362c0097644797b38de6f3d036f79a",
            "2af9bce552b14ed6bcc4336cfec4301c",
            "c319707f0a0b4f299183c10a317f4650",
            "b9e0f10a84104d6099021146647b0a79",
            "f327d7373c89471eb1779a1863a6b49d",
            "5e22fb387b514ca59e9c5cc00fbc889e",
            "e6d0d95b95304e4da1e799803a440789",
            "1c0a59ade8164244bb3a2018c1b1c8fa",
            "ffd8b8c92f5a4662b68970bb53f2c66d",
            "8529104f19504ac19d7c2de114ce862d",
            "4d33693a2fb24d4e8ad12d7c4a7fbb9d",
            "7b0d064c63c941aebe80b88f6591bda9",
            "86bc781e969a4881a703481fc9ee19b8",
            "7af754b77fb24a4594096d70d79c2837",
            "3e83c1a9a32c496d8ef008f87bc01906",
            "a0bb36f408c24608a003906f67d2689c",
            "9d2813e474d14e48926428f779c65e2b",
            "c6388c547bc94b0986f5ac79427255e1",
            "894e1eea2fac4c78b6d806ffc735eb26",
            "70410c7584ac4d5587843485085aa81f",
            "f50c5b156efc4c2488e471214997cbed",
            "e6072f63eebe42fb8fc3d00b4f1e4477",
            "7e44d9a5cc774ba6942c586f221984ec",
            "30e4d2e1cf834ea4a3b408d9bd7f8986",
            "0cae9544de574aed99d7cc8fa039e568",
            "269a47143b1740bb9ad4c31986914674",
            "847159ae1bc94652890b49edc1fb9866",
            "0fc61928a6e540f6bf0b6ebb32bdf218",
            "b3f525e872cd412d9fe7e0fafa38d59a",
            "8b28e25267b745c892ac3a2ba7fc23b8",
            "17c2bc4d0e404f549ba0b249fa632389",
            "87db41f135814fa3830b31ed406841d2",
            "722804ee8cce4cc4ba8ef8cb317e6018",
            "ea8620518f0445aea8f693f722469d48",
            "5eff9aa916e2411c8f861b3215af0165",
            "7c3f851fc37546508b4ae5c360d4a108",
            "0a850bb17f184af5a12659419117de90",
            "a572049b34c54c7cb5568203376fa65a",
            "adecc51b9a544d81991263107840e8d2",
            "f95758b1a4f4487fb85c8de574187d7f",
            "bdee18f772f44334839c20af65a9425b",
            "0363fb7cae934d4180b9086168b30f67",
            "b1c6c20419474a69b33f7fe64bb6c7a8",
            "3c3edc68be64450297404f97d2f4d6f8",
            "03b150eb3a744bcd8b2d9a31c04b495a",
            "027cc4e8a7b94528a2861b84a74ebf61",
            "f63a499225d24b5bb8667198194fd8a9",
            "3841e5eb36f548879995518dfed6abb5",
            "9d14cde255f849849fe646d0da5f630c",
            "da97b9a98c63460fa887be6b3f68f615",
            "3a5757be7b734ff5aa81aadadcf48221",
            "10762d9a95cf4c3ea5dc9392b3df5edf",
            "b2c457b9bff74f8f968ea71ccb8cf3a6",
            "55ef889465154cb7bbe03f85fdc3a58f",
            "ed75824d95e14db2988e828dc57e5cc3",
            "639bd5a0d3cd42898e8710471959a230",
            "6670d704faaf457aad1a421a82b650b7",
            "d44da8914bb24c70b778c1a0e131062c",
            "8a88d34df75b4e4bbeb3d4e687bf2ca6",
            "67b1de93abaa4b73b22fa327ffa2de1e",
            "40df3d1b7bf743989cbfeec0535a0ecd",
            "b1fdd008ea1540e282ba3a46b9f0f6a9",
            "c6c1bd6f386c4aee98d8c98c542369a3",
            "879c921201b04800b5c0de6b21b4ef79",
            "daa8c56b26c548e199dcfdf2364f8a02",
            "9d711af7b1074e4d801987ca4d263e23",
            "847c19aa91d94962b003af1aa0317143",
            "9f0f335dc297438fbe971e811e1d0e81",
            "f4223645613b47658d4e6f7fe80bf179",
            "f7a137d21be64a918bd1ddf4f9b1a4d9",
            "beff5e0595ad4c93b7235fcf4bc62a4d",
            "a9d39adf81b048f69a16e1206c42412f",
            "4bd90052382848af936f58964b9b8eaa",
            "e02af1133b04466d80d5b97d307ba80b",
            "74243074db0b416d83e9150b60cd569c",
            "19eb6a4b4d9c48fb8a6e891e19d47d08",
            "f5f92da85e384f048265190107d1d960",
            "765698ffadd346aaa86839184ed752e2",
            "841ebdeb9d6d4c5ea1df324fb573c9de",
            "09f4681a66e041ad954b0c0da3555ece",
            "0e6f2443da83469d95ff138fad02e2f9",
            "2e9c9b9167f440a3a10c13cb7b237415",
            "baf917ea5ea943febc7be9ab3d19bfc3",
            "54f6530ebb794da1b81e566be2d0968d",
            "c76cf5c369f441c5a5dac99174184b2e",
            "8dd09d74fa4d4389a82463e9f1f4724e",
            "53043589e623494faad48e2f25d6351e",
            "81f05baed0884ba68971dd2a6a99da58",
            "dd760ae6d2f74b6ca2d5f0a217a76a0e",
            "f296f5f69a0242c3b0e712bc3910fe97",
            "fb8cb58710844e71be596705253da265",
            "7c32e65cd0b94262b2d04a5e8c6a8e45",
            "3e95f6d5a4634173b1cb9de88ac74736",
            "301cf91761d1400cafe56693dbfda818",
            "973d4ab2a8b240aba8fc1f0fa9aa1f48",
            "21ab6bcca2d34653bd3c22c537d6e10c",
            "111f0e5b2d444a1fbf822e86af3827d1",
            "da7208ab53ba412dbde141089dbf1658",
            "1000ee9e6f1b4f198d87acd8b0d69278",
            "e858865100a74aa183fa7e65a6de25a4",
            "f38633162a804721bbeda384cd401df7",
            "83d8b1b3898f4c04921d374dbe6b5478",
            "bfacc4dbcc8e41728bd5ba5cf2164f82",
            "ce3f0a3ccb72409e8b3cbac0ff92e9c2",
            "1e311f4ec7a240a28ca00225620929b7",
            "c860358e148f4093a00d93e64ec518e5",
            "633e1cf8df63445fb5dcebaddf8019c0",
            "6296d05e28504f31a0aafc8d2afdcfea",
            "d6e71de56a2c4766a6909539e1541987",
            "68562bd734cc460395644905dbf609e8",
            "77ab119add1e4de9a7f43dd639364d81"
          ]
        },
        "id": "aqLPHKRQNQqE",
        "outputId": "c368560a-c679-4bab-ac70-c8cc7692cf3a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e585cdd9062c4d6b995405ab3f304f17"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2af9bce552b14ed6bcc4336cfec4301c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86bc781e969a4881a703481fc9ee19b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30e4d2e1cf834ea4a3b408d9bd7f8986"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5eff9aa916e2411c8f861b3215af0165"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "027cc4e8a7b94528a2861b84a74ebf61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6670d704faaf457aad1a421a82b650b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f0f335dc297438fbe971e811e1d0e81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "841ebdeb9d6d4c5ea1df324fb573c9de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f296f5f69a0242c3b0e712bc3910fe97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f38633162a804721bbeda384cd401df7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense top-3: [('p0024', 'بچوں میں کووڈ-19 عام طور پر ہلکا ہوتا ہے مگر بعض نادر معاملات میں شدید علامات سامنے آ سکتی ہیں؛ بچوں کے لیے مخصوص رہنمائی مختلف ہو سکتی ہے۔', 0.7648560404777527), ('p0021', 'کووڈ-19 کے بعد بعض افراد میں طویل مدتی علامات (Long COVID) جیسے تھکن، سانس کی تکلیف اور دماغی دھند برقرار رہ سکتی ہیں؛ ریہیب پروگرامز مدد دیتے ہیں۔', 0.6735703349113464), ('p0036', 'کووڈ-19 کے مریضوں میں خون جمنے کے مسائل اور دیگر پیچیدگیاں بعض اوقات سامنے آئیں، اس لیے طبی نگرانی اور مناسب علاج ضروری ہے۔', 0.647298276424408)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6b: Evaluation of dense retriever (run after Cell 6)\n",
        "# Purpose: measure Recall@1, Recall@5, MRR, Precision@k, latency for dense_retrieve\n",
        "# Uses eval_queries with \"positive_ids\" and \"gold_answer\" fields\n",
        "\n",
        "import json, time, re, statistics\n",
        "\n",
        "OUT_JSONL_DENSE = \"dense_eval_results.jsonl\"\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def normalize_text(s):\n",
        "    if s is None: return \"\"\n",
        "    return re.sub(r\"\\s+\", \" \", str(s).strip())\n",
        "\n",
        "def get_query_text(item):\n",
        "    return item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "\n",
        "def evaluate_dense(eval_items, out_jsonl=OUT_JSONL_DENSE, k=DEFAULT_K,\n",
        "                   recall_ks=RECALL_KS, precision_ks=PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies, rr_list = [], []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in eval_items:\n",
        "        total += 1\n",
        "        q = get_query_text(item)\n",
        "        pos_ids = item.get(\"positive_ids\") or []\n",
        "        if isinstance(pos_ids, str): pos_ids = [pos_ids]\n",
        "        pos_ids = [str(x) for x in pos_ids]\n",
        "\n",
        "        gold_text = normalize_text(item.get(\"gold_answer\") or \"\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        hits = dense_retrieve(q, k=k)  # (id, text, score)\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [h[0] for h in hits]\n",
        "        retrieved_texts = [h[1] for h in hits]\n",
        "\n",
        "        # Reciprocal rank\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in pos_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in pos_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in pos_ids)\n",
        "            precision_sums[pk] += num_pos_in_topk / pk\n",
        "\n",
        "        per_query.append({\n",
        "            \"query_id\": item.get(\"query_id\"),\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": pos_ids,\n",
        "            \"gold_text\": gold_text,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"retrieved_texts_preview\": [txt[:300] for txt in retrieved_texts],\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": sum(rr_list)/n,\n",
        "        **{f\"Recall@{rk}\": recall_counts[rk]/n for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_sums[pk]/n for pk in precision_ks},\n",
        "        \"latency_mean_s\": statistics.mean(latencies) if latencies else 0.0,\n",
        "        \"latency_median_s\": statistics.median(latencies) if latencies else 0.0\n",
        "    }\n",
        "\n",
        "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in per_query:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# Run evaluation\n",
        "print(\"[dense_eval] Running dense retriever evaluation...\")\n",
        "summary_dense, records_dense = evaluate_dense(eval_queries, out_jsonl=OUT_JSONL_DENSE, k=DEFAULT_K)\n",
        "print(\"\\nDense retriever evaluation summary:\")\n",
        "for k,v in summary_dense.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Show a few examples\n",
        "print(\"\\nExamples (first 5):\")\n",
        "for r in records_dense[:5]:\n",
        "    print(\" - Query:\", r[\"query\"][:80])\n",
        "    print(\"   Retrieved ids:\", r[\"retrieved_ids\"][:6])\n",
        "    print(\"   Reciprocal rank:\", r[\"reciprocal_rank\"], \"Latency(s):\", round(r[\"latency\"], 4))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEmXvrvZNY9g",
        "outputId": "0a3f6688-21eb-4f8a-c15c-30bafa47a7e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[dense_eval] Running dense retriever evaluation...\n",
            "\n",
            "Dense retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.8386666666666666\n",
            "  Recall@1: 0.74\n",
            "  Recall@5: 0.96\n",
            "  Precision@1: 0.74\n",
            "  Precision@5: 0.21599999999999964\n",
            "  latency_mean_s: 0.009761817455291748\n",
            "  latency_median_s: 0.009441375732421875\n",
            "\n",
            "Examples (first 5):\n",
            " - Query: کووڈ-19 کی عام علامات کیا ہیں؟\n",
            "   Retrieved ids: ['p0024', 'p0021', 'p0001', 'p0036', 'p0044']\n",
            "   Reciprocal rank: 0.3333333333333333 Latency(s): 0.015\n",
            "\n",
            " - Query: کووڈ-19 کی تشخیص کے لیے کون سا ٹیسٹ عام طور پر استعمال ہوتا ہے؟\n",
            "   Retrieved ids: ['p0018', 'p0002', 'p0036', 'p0045', 'p0024']\n",
            "   Reciprocal rank: 0.5 Latency(s): 0.0207\n",
            "\n",
            " - Query: ہاتھوں کی صفائی وبا کے دوران کیوں ضروری ہے؟\n",
            "   Retrieved ids: ['p0030', 'p0003', 'p0042', 'p0041', 'p0020']\n",
            "   Reciprocal rank: 0.5 Latency(s): 0.0139\n",
            "\n",
            " - Query: ماسک پہننے کے کیا فوائد ہیں؟\n",
            "   Retrieved ids: ['p0004', 'p0029', 'p0022', 'p0042', 'p0050']\n",
            "   Reciprocal rank: 1.0 Latency(s): 0.0097\n",
            "\n",
            " - Query: سماجی فاصلہ رکھنے کی اہمیت کیا ہے؟\n",
            "   Retrieved ids: ['p0005', 'p0054', 'p0016', 'p0015', 'p0043']\n",
            "   Reciprocal rank: 1.0 Latency(s): 0.0103\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Prepare InputExamples for sentence-transformers fine-tuning i.e. of dense retriever model\n",
        "# Now with an 80/20 train/validation split\n",
        "\n",
        "from sentence_transformers import InputExample\n",
        "import random\n",
        "\n",
        "pid2text = {p[\"id\"]: p[\"text\"] for p in passages_min}\n",
        "\n",
        "examples = []\n",
        "for s in synthetic_pairs:\n",
        "    q = s[\"query\"]\n",
        "    pos = pid2text.get(s[\"positive_id\"])\n",
        "    neg = None\n",
        "    # Find hard negatives if available\n",
        "    hn = next((h for h in hard_negatives if h[\"query_id\"] == s.get(\"synthetic_id\", s.get(\"query_id\"))), None)\n",
        "    if hn:\n",
        "        for nid in hn[\"hard_negatives\"]:\n",
        "            if nid != s[\"positive_id\"]:\n",
        "                neg = pid2text.get(nid)\n",
        "                break\n",
        "    if neg is None:\n",
        "        # fallback: random negative\n",
        "        neg_id = random.choice([pid for pid in corpus_ids if pid != s[\"positive_id\"]])\n",
        "        neg = pid2text[neg_id]\n",
        "    if pos and neg:\n",
        "        examples.append(InputExample(texts=[q, pos, neg]))\n",
        "\n",
        "print(\"Prepared\", len(examples), \"triplet examples.\")\n",
        "\n",
        "# --- Split into train/validation (80/20) ---\n",
        "random.shuffle(examples)\n",
        "split_idx = int(0.8 * len(examples))\n",
        "train_examples = examples[:split_idx]\n",
        "val_examples = examples[split_idx:]\n",
        "\n",
        "print(\"Train examples:\", len(train_examples))\n",
        "print(\"Validation examples:\", len(val_examples))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct83mG2wNjlG",
        "outputId": "d266121e-890d-4070-9028-9cdaf9475091"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 500 triplet examples.\n",
            "Train examples: 400\n",
            "Validation examples: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 (use in-memory model; do NOT reload): Fine-tune SBERT with triplet loss and IR validation on passages_min\n",
        "import os\n",
        "# --- GRANDMASTER FIX: DISABLE WANDB ---\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "# --------------------------------------\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import SentenceTransformer, losses, evaluation\n",
        "#import faiss\n",
        "\n",
        "# Sanity checks\n",
        "assert isinstance(train_examples, list) and len(train_examples) > 0, \"train_examples must be a non-empty list\"\n",
        "assert 'passages_min' in globals(), \"passages_min must be loaded\"\n",
        "assert 'eval_queries' in globals(), \"eval_queries must be loaded\"\n",
        "\n",
        "# Build validation split against real corpus & labels\n",
        "# (Check if eval_queries_val exists, otherwise split eval_queries)\n",
        "eval_val = eval_queries_val if 'eval_queries_val' in globals() else eval_queries[int(0.8*len(eval_queries)):]\n",
        "\n",
        "val_queries_dict = {it[\"query_id\"]: it[\"query\"] for it in eval_val}\n",
        "# Fix: Ensure positive_ids is a list\n",
        "val_relevant_dict = {it[\"query_id\"]: set(it[\"positive_ids\"] if isinstance(it[\"positive_ids\"], list) else [it[\"positive_ids\"]]) for it in eval_val}\n",
        "val_corpus_dict = {p[\"id\"]: p[\"text\"] for p in passages_min}\n",
        "\n",
        "# Warn if labels reference missing ids\n",
        "missing = []\n",
        "for qid, rels in val_relevant_dict.items():\n",
        "    for pid in rels:\n",
        "        if pid not in val_corpus_dict:\n",
        "            missing.append((qid, pid))\n",
        "if missing:\n",
        "    print(f\"Warning: {len(missing)} relevant ids not found in corpus. Example:\", missing[:3])\n",
        "\n",
        "# Construct evaluator (defaults to cosine similarity)\n",
        "retrieval_evaluator = evaluation.InformationRetrievalEvaluator(\n",
        "    queries=val_queries_dict,\n",
        "    corpus=val_corpus_dict,\n",
        "    relevant_docs=val_relevant_dict,\n",
        "    name=\"val_ir_passages\"\n",
        ")\n",
        "\n",
        "# Start from baseline multilingual MiniLM\n",
        "# We use the variable 'embedder' from Cell 6 to ensure we continue correctly\n",
        "if 'embedder' not in globals():\n",
        "    embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "embedder.to(\"cuda\")\n",
        "\n",
        "# Triplet loss with conservative settings\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "train_loss = losses.TripletLoss(\n",
        "    model=embedder,\n",
        "    distance_metric=losses.TripletDistanceMetric.COSINE,\n",
        "    triplet_margin=0.3\n",
        ")\n",
        "\n",
        "num_epochs = 2\n",
        "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)\n",
        "optimizer_params = {'lr': 2e-5}\n",
        "\n",
        "print(\"Starting fine-tuning (WandB Disabled)...\")\n",
        "\n",
        "# Train with IR evaluator\n",
        "embedder.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    evaluator=retrieval_evaluator,\n",
        "    epochs=num_epochs,\n",
        "    warmup_steps=warmup_steps,\n",
        "    optimizer_params=optimizer_params,\n",
        "    show_progress_bar=True,\n",
        "    output_path=\"fine_tuned_sbert_urdu_passages\"\n",
        ")\n",
        "\n",
        "print(\"✅ Fine-tuning complete. Using in-memory fine-tuned 'embedder' (no reload).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279,
          "referenced_widgets": [
            "9b6a23a41bea400983e7a4c52447c623",
            "afcde295bbdb4a77b89bdd7df4df9100",
            "28b02f49690949f09159402002f6a7ce",
            "99697e7d856b4dd29f46c438b9df5104",
            "2889a4bfc43047e188f69742e46c945f",
            "9cf41fc44fda44f8a176e1b410183b6a",
            "d6867b7332e1462c9e0d340281c0a111",
            "cd30e10602cf41229e9c2c1f3229aa6b",
            "3e11687e6e8d47cebb9fbe5aeaa97cd8",
            "1373ea00dfaf4a9f92ae6b0acd8d8530",
            "3ea34b50bb91463b85e7dcf8f1db9bc3"
          ]
        },
        "id": "b_6sBHCzNmKY",
        "outputId": "3160f4c2-d88b-4c30-b798-9b4d51672fc7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fine-tuning (WandB Disabled)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b6a23a41bea400983e7a4c52447c623"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:18, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Val Ir Passages Cosine Accuracy@1</th>\n",
              "      <th>Val Ir Passages Cosine Accuracy@3</th>\n",
              "      <th>Val Ir Passages Cosine Accuracy@5</th>\n",
              "      <th>Val Ir Passages Cosine Accuracy@10</th>\n",
              "      <th>Val Ir Passages Cosine Precision@1</th>\n",
              "      <th>Val Ir Passages Cosine Precision@3</th>\n",
              "      <th>Val Ir Passages Cosine Precision@5</th>\n",
              "      <th>Val Ir Passages Cosine Precision@10</th>\n",
              "      <th>Val Ir Passages Cosine Recall@1</th>\n",
              "      <th>Val Ir Passages Cosine Recall@3</th>\n",
              "      <th>Val Ir Passages Cosine Recall@5</th>\n",
              "      <th>Val Ir Passages Cosine Recall@10</th>\n",
              "      <th>Val Ir Passages Cosine Ndcg@10</th>\n",
              "      <th>Val Ir Passages Cosine Mrr@10</th>\n",
              "      <th>Val Ir Passages Cosine Map@100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.525000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.810604</td>\n",
              "      <td>0.879167</td>\n",
              "      <td>0.751938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>0.825000</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.789570</td>\n",
              "      <td>0.851667</td>\n",
              "      <td>0.742094</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fine-tuning complete. Using in-memory fine-tuned 'embedder' (no reload).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8b: Save the Fine-Tuned Model to Drive (Run ONLY if satisfied with accuracy)\n",
        "import os\n",
        "\n",
        "# Define path\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/models/urdu_dense_retriever_best\"\n",
        "\n",
        "print(f\"💾 Saving model to {MODEL_SAVE_PATH} ...\")\n",
        "\n",
        "# Create directory if not exists\n",
        "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# Save the model\n",
        "embedder.save(MODEL_SAVE_PATH)\n",
        "\n",
        "print(f\"✅ Model saved! You can now use Cell 8c in future sessions to skip training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp87l4-mNpmv",
        "outputId": "a3b76757-3205-4f90-ba71-95129661a937"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saving model to /content/drive/MyDrive/models/urdu_dense_retriever_best ...\n",
            "✅ Model saved! You can now use Cell 8c in future sessions to skip training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8c: FAST START - Load Model from Drive & Rebuild FAISS (Skips Training)\n",
        "# Run this INSTEAD of Cells 6, 7, 8, 8b in future sessions.\n",
        "\n",
        "import os\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/models/urdu_dense_retriever_best\"\n",
        "\n",
        "# 1. Load the Model\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    print(f\"📂 Loading saved model from: {MODEL_SAVE_PATH}\")\n",
        "    embedder = SentenceTransformer(MODEL_SAVE_PATH).to(\"cuda\")\n",
        "    print(\"✅ Model loaded successfully.\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"❌ No saved model found at {MODEL_SAVE_PATH}. Please run Cell 8 & 8b once to create it!\")\n",
        "\n",
        "# 2. Rebuild FAISS Index (Critical Step)\n",
        "# We must re-encode the corpus because we just loaded a specific model\n",
        "print(\"⏳ Generating embeddings for corpus...\")\n",
        "corpus_texts = [p[\"text\"] for p in passages_min]\n",
        "\n",
        "# Generate embeddings\n",
        "passage_embeddings = embedder.encode(corpus_texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# Build FAISS\n",
        "faiss.normalize_L2(passage_embeddings)\n",
        "index = faiss.IndexFlatIP(passage_embeddings.shape[1])\n",
        "index.add(passage_embeddings)\n",
        "\n",
        "# 3. Define the Retrieval Function\n",
        "# (We must re-define this here because we skipped the previous cells that defined it)\n",
        "def dense_retrieve(query, k=5):\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    results = []\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "        results.append((corpus_ids[idx], corpus_texts[idx], float(score)))\n",
        "    return results\n",
        "\n",
        "print(\"✅ Dense Retriever System Restored & Ready for Hybrid Fusion (Cell 9).\")"
      ],
      "metadata": {
        "id": "cv3cf_97Nr1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now run cell 6b again to test the improvement of our dense retriever model after fine tuning."
      ],
      "metadata": {
        "id": "ixajsvTkNwDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 (final): Retriever wrapper with true fusion modes (non-destructive)\n",
        "# - Creates bm25_new only if not present\n",
        "# - Supports modes: 'bm25', 'dense', 'hybrid_interleave' (legacy), 'hybrid_score', 'hybrid_rrf'\n",
        "# - Returns list of (pid, text, score) tuples\n",
        "# - Does NOT rebuild or overwrite dense/FAISS objects\n",
        "\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# ---------- Config ----------\n",
        "# Tune these later on a small validation set\n",
        "DEFAULT_RETRIEVE_POOL = 50\n",
        "SCORE_FUSION_ALPHA = 0.6   # alpha in [0,1] for score fusion: alpha * dense + (1-alpha) * bm25\n",
        "RRF_K = 60                 # reciprocal rank fusion constant\n",
        "\n",
        "# ---------- Sanity checks for canonical corpus ----------\n",
        "assert 'passages_min' in globals() and isinstance(passages_min, list) and len(passages_min) > 0, \"passages_min must be loaded\"\n",
        "assert 'pid2text' in globals() and isinstance(pid2text, dict) and len(pid2text) > 0, \"pid2text must be available\"\n",
        "assert 'dense_retrieve' in globals(), \"dense_retrieve wrapper must be defined (fine-tuned dense retriever)\"\n",
        "\n",
        "# ---------- Build or reuse BM25 index (non-destructive) ----------\n",
        "try:\n",
        "    # If bm25_new already exists from a previous run, reuse it\n",
        "    bm25_new  # noqa: F821\n",
        "except Exception:\n",
        "    try:\n",
        "        from rank_bm25 import BM25Okapi\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"rank_bm25\"], check=True)\n",
        "        from rank_bm25 import BM25Okapi\n",
        "\n",
        "    # Build tokenized corpus from passages_min (light normalization)\n",
        "    def _normalize_for_bm25(s: str) -> str:\n",
        "        if s is None:\n",
        "            return \"\"\n",
        "        s = s.replace(\"\\u200c\", \" \")  # zero-width non-joiner\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "        return s.lower()\n",
        "\n",
        "    bm25_new_ids = [p[\"id\"] for p in passages_min]\n",
        "    bm25_new_texts = [p[\"text\"] for p in passages_min]\n",
        "    bm25_new_tokenized = [_normalize_for_bm25(t).split() for t in bm25_new_texts]\n",
        "    bm25_new = BM25Okapi(bm25_new_tokenized)\n",
        "\n",
        "# Safe wrapper for BM25 that returns (pid, text, score)\n",
        "def bm25_new_retrieve(query: str, k: int = 5):\n",
        "    q_tok = _normalize_for_bm25(query).split()\n",
        "    scores = bm25_new.get_scores(q_tok)\n",
        "    top_idx = np.argsort(scores)[::-1][:k]\n",
        "    results = []\n",
        "    for i in top_idx:\n",
        "        i = int(i)\n",
        "        pid = bm25_new_ids[i]\n",
        "        text = bm25_new_texts[i]\n",
        "        score = float(scores[i])\n",
        "        results.append((pid, text, score))\n",
        "    return results\n",
        "\n",
        "# ---------- Fusion utilities ----------\n",
        "def normalize_scores(score_map):\n",
        "    \"\"\"Min-max normalize a dict of scores to [0,1].\"\"\"\n",
        "    if not score_map:\n",
        "        return {}\n",
        "    vals = list(score_map.values())\n",
        "    lo, hi = min(vals), max(vals)\n",
        "    if hi == lo:\n",
        "        return {k: 1.0 for k in score_map}\n",
        "    return {k: (v - lo) / (hi - lo) for k, v in score_map.items()}\n",
        "\n",
        "def rrf_rank(dense_list, bm25_list, k_rrf=RRF_K):\n",
        "    \"\"\"Reciprocal Rank Fusion: returns sorted list of pids by RRF score.\"\"\"\n",
        "    score = {}\n",
        "    for rank, (pid, _, _) in enumerate(dense_list, start=1):\n",
        "        score[pid] = score.get(pid, 0.0) + 1.0 / (k_rrf + rank)\n",
        "    for rank, (pid, _, _) in enumerate(bm25_list, start=1):\n",
        "        score[pid] = score.get(pid, 0.0) + 1.0 / (k_rrf + rank)\n",
        "    sorted_pids = sorted(score.keys(), key=lambda p: score[p], reverse=True)\n",
        "    return sorted_pids, score\n",
        "\n",
        "# ---------- Metadata filter helper (unchanged semantics) ----------\n",
        "# If you have meta_map from corpus_clean, it will be used; otherwise fallback to passages_min metadata\n",
        "if 'corpus_clean' in globals():\n",
        "    meta_map = {p[\"id\"]: p for p in corpus_clean}\n",
        "else:\n",
        "    meta_map = {p[\"id\"]: p for p in passages_min}\n",
        "\n",
        "def filter_by_metadata(candidate_ids, min_date=None, max_date=None, allowed_sources=None, exclude_time_sensitive=None):\n",
        "    out = []\n",
        "    for pid in candidate_ids:\n",
        "        m = meta_map.get(pid, {})\n",
        "        ok = True\n",
        "        if min_date or max_date:\n",
        "            dt = None\n",
        "            if \"retrieved_at\" in m:\n",
        "                try:\n",
        "                    dt = datetime.fromisoformat(m[\"retrieved_at\"])\n",
        "                except Exception:\n",
        "                    dt = None\n",
        "            if dt:\n",
        "                if min_date and dt < min_date: ok = False\n",
        "                if max_date and dt > max_date: ok = False\n",
        "        if allowed_sources and m.get(\"source\") not in allowed_sources:\n",
        "            ok = False\n",
        "        if exclude_time_sensitive is not None and m.get(\"time_sensitive\") == exclude_time_sensitive:\n",
        "            ok = False\n",
        "        if ok:\n",
        "            out.append(pid)\n",
        "    return out\n",
        "\n",
        "# ---------- Main retrieve wrapper with fusion modes ----------\n",
        "def retrieve(query: str, k: int = 5, mode: str = \"hybrid_score\", min_date=None, max_date=None, allowed_sources=None, exclude_time_sensitive=None):\n",
        "    \"\"\"\n",
        "    retrieve(query, k, mode)\n",
        "    Modes:\n",
        "      - 'bm25' : BM25-only (bm25_new_retrieve)\n",
        "      - 'dense' : dense-only (dense_retrieve)\n",
        "      - 'hybrid_interleave' : legacy interleave (dense first, then bm25)\n",
        "      - 'hybrid_score' : score fusion (normalized dense + bm25)\n",
        "      - 'hybrid_rrf' : reciprocal rank fusion (RRF)\n",
        "    Returns: list of (pid, text, score)\n",
        "    \"\"\"\n",
        "    # Get candidate pools (pool size configurable)\n",
        "    pool = max(DEFAULT_RETRIEVE_POOL, k)\n",
        "    dense_hits = dense_retrieve(query, k=pool)   # expected (pid, text, score)\n",
        "    bm25_hits = bm25_new_retrieve(query, k=pool) # (pid, text, score)\n",
        "\n",
        "    # Mode-specific behavior\n",
        "    if mode == \"bm25\":\n",
        "        results = bm25_hits[:k]\n",
        "    elif mode == \"dense\":\n",
        "        results = dense_hits[:k]\n",
        "    elif mode == \"hybrid_interleave\":\n",
        "        # preserve dense-first interleaving (legacy behavior)\n",
        "        seen = set()\n",
        "        cands = []\n",
        "        for lst in (dense_hits, bm25_hits):\n",
        "            for pid, text, score in lst:\n",
        "                if pid not in seen:\n",
        "                    seen.add(pid)\n",
        "                    cands.append((pid, text, float(score)))\n",
        "        results = cands[:k]\n",
        "    elif mode == \"hybrid_score\":\n",
        "        # Score fusion: normalize and combine\n",
        "        dense_scores = {pid: sc for pid, _, sc in dense_hits}\n",
        "        bm25_scores = {pid: sc for pid, _, sc in bm25_hits}\n",
        "        d_norm = normalize_scores(dense_scores)\n",
        "        b_norm = normalize_scores(bm25_scores)\n",
        "        alpha = SCORE_FUSION_ALPHA\n",
        "        combined = {}\n",
        "        for pid in set(list(d_norm.keys()) + list(b_norm.keys())):\n",
        "            combined[pid] = alpha * d_norm.get(pid, 0.0) + (1 - alpha) * b_norm.get(pid, 0.0)\n",
        "        # sort by combined score\n",
        "        sorted_pids = sorted(combined.keys(), key=lambda p: combined[p], reverse=True)\n",
        "        results = []\n",
        "        for pid in sorted_pids[:k]:\n",
        "            text = pid2text.get(pid, next((p[\"text\"] for p in passages_min if p[\"id\"] == pid), \"\"))\n",
        "            results.append((pid, text, float(combined[pid])))\n",
        "    elif mode == \"hybrid_rrf\":\n",
        "        sorted_pids, score_map = rrf_rank(dense_hits, bm25_hits, k_rrf=RRF_K)\n",
        "        results = []\n",
        "        for pid in sorted_pids[:k]:\n",
        "            text = pid2text.get(pid, next((p[\"text\"] for p in passages_min if p[\"id\"] == pid), \"\"))\n",
        "            results.append((pid, text, float(score_map.get(pid, 0.0))))\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown retrieve mode: {mode}\")\n",
        "\n",
        "    # Apply metadata filters if requested (filter by pid only)\n",
        "    if any([min_date, max_date, allowed_sources, exclude_time_sensitive is not None]):\n",
        "        filtered_ids = filter_by_metadata([pid for pid,_,_ in results], min_date, max_date, allowed_sources, exclude_time_sensitive)\n",
        "        results = [(pid, pid2text.get(pid, \"\"), score) for pid,_,score in results if pid in filtered_ids]\n",
        "\n",
        "    return results\n",
        "\n",
        "# ---------- Quick sample test (safe) ----------\n",
        "q = eval_queries[0][\"query\"] if 'eval_queries' in globals() and len(eval_queries)>0 else \"کووڈ-19 کی عام علامات کیا ہیں؟\"\n",
        "print(\"Sample dense top-5 ids:\", [r[0] for r in dense_retrieve(q, k=5)])\n",
        "print(\"Sample bm25_new top-5 ids:\", [r[0] for r in bm25_new_retrieve(q, k=5)])\n",
        "print(\"Sample hybrid_score top-5 ids:\", [r[0] for r in retrieve(q, k=5, mode='hybrid_score')])\n",
        "print(\"Sample hybrid_rrf top-5 ids:\", [r[0] for r in retrieve(q, k=5, mode='hybrid_rrf')])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkZxB9ZFNtiP",
        "outputId": "f0f6efc7-b698-4eda-ae8f-e64f7317f6d9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample dense top-5 ids: ['p0024', 'p0021', 'p0001', 'p0036', 'p0044']\n",
            "Sample bm25_new top-5 ids: ['p0001', 'p0024', 'p0002', 'p0044', 'p0021']\n",
            "Sample hybrid_score top-5 ids: ['p0024', 'p0001', 'p0021', 'p0044', 'p0002']\n",
            "Sample hybrid_rrf top-5 ids: ['p0024', 'p0001', 'p0021', 'p0044', 'p0002']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9b: Run after above cell 9. Cell 9 creates B2M5 + Dense hybrid and below\n",
        "# cell evaluates its performance:\n",
        "# Validation diagnostics — Recall@1, Recall@5, MRR, Precision@k for retrievers\n",
        "# - Works with any mode supported by your Cell 9 wrapper: 'bm25', 'dense', 'hybrid_interleave', 'hybrid_score', 'hybrid_rrf'\n",
        "# - Calls retrieve(...) and computes retrieval metrics\n",
        "# - Outputs summary metrics and a few examples of misses\n",
        "\n",
        "import time, statistics\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def evaluate_retriever(eval_items, mode=\"hybrid_score\", k=DEFAULT_K,\n",
        "                       recall_ks=RECALL_KS, precision_ks=PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies = []\n",
        "    rr_list = []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in tqdm(eval_items, desc=f\"Evaluating {mode} retriever\"):\n",
        "        total += 1\n",
        "        q = item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "        positive_ids = item.get(\"positive_ids\") or item.get(\"positive_id\") or []\n",
        "        if isinstance(positive_ids, str):\n",
        "            positive_ids = [positive_ids]\n",
        "        positive_ids = [str(x) for x in positive_ids]\n",
        "\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            hits = retrieve(q, k=k, mode=mode)   # (pid, text, score)\n",
        "        except Exception as e:\n",
        "            hits = []\n",
        "            print(f\"[eval] retrieve error for query {q[:60]}... -> {e}\")\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [r[0] for r in hits]\n",
        "\n",
        "        # Reciprocal rank: first position among positives\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in positive_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in positive_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in positive_ids)\n",
        "            precision_sums[pk] += (num_pos_in_topk / pk)\n",
        "\n",
        "        per_query.append({\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": positive_ids,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    mrr = sum(rr_list) / n\n",
        "    recall_at = {rk: recall_counts[rk] / n for rk in recall_ks}\n",
        "    precision_at = {pk: precision_sums[pk] / n for pk in precision_ks}\n",
        "    latency_mean = statistics.mean(latencies) if latencies else 0.0\n",
        "    latency_median = statistics.median(latencies) if latencies else 0.0\n",
        "\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": mrr,\n",
        "        **{f\"Recall@{rk}\": recall_at[rk] for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_at[pk] for pk in precision_ks},\n",
        "        \"latency_mean_s\": latency_mean,\n",
        "        \"latency_median_s\": latency_median\n",
        "    }\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# ---------- Run evaluation ----------\n",
        "# Use eval_queries_val if defined, else fall back to eval_queries\n",
        "eval_items = eval_queries_val if 'eval_queries_val' in globals() else eval_queries\n",
        "\n",
        "# Evaluate all retriever modes\n",
        "modes = [\"bm25\", \"dense\", \"hybrid_interleave\", \"hybrid_score\", \"hybrid_rrf\"]\n",
        "results = {}\n",
        "for m in modes:\n",
        "    summary, records = evaluate_retriever(eval_items, mode=m, k=DEFAULT_K)\n",
        "    results[m] = summary\n",
        "    if m == \"dense\":\n",
        "      print(f\"\\n{m} retriever (Fine-tuned) evaluation summary:\")\n",
        "    else:\n",
        "      print(f\"\\n{m} retriever evaluation summary:\")\n",
        "    for k,v in summary.items():\n",
        "        print(f\"  {k}: {v:.3f}\" if isinstance(v,float) else f\"  {k}: {v}\")\n",
        "\n",
        "    # Show a few misses\n",
        "    misses = [r for r in records if r[\"reciprocal_rank\"] == 0.0]\n",
        "    print(f\"  Total misses: {len(misses)} / {len(records)}. Showing up to 3 misses:\")\n",
        "    for r in misses[:3]:\n",
        "        print(\"   Query:\", r[\"query\"][:80])\n",
        "        print(\"    Positives:\", r[\"positive_ids\"])\n",
        "        print(\"    Retrieved top ids:\", r[\"retrieved_ids\"][:8])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8a401930b827456a8ad83a15b26fd591",
            "f60e19b3a23243498e60115d398df456",
            "5e4e2f551335473cbd69586faf9db609",
            "ea9c32d45d274e079583c2b9c39ab68d",
            "a019a389fa75472a9e3c43c9235bba49",
            "b55016616f0f4a4c92f028a87b203cc1",
            "0d3ccbb0ec8647e0bddd07a4f5113ece",
            "d0be47ae7c2542daad1e351183c597fe",
            "8bc73c46537149318a31d96d61660822",
            "49c0e0a06dd849098b7aedd9d95e1b1b",
            "2977cc34bc814bd195b4629077449c69",
            "669e6f361a494787a4fd4fde7937cc3c",
            "ca394784f6f5417498c202724849aa6f",
            "f1dcc3727fd3472ba315c19d72ce1f73",
            "1d162ccb7fcc40c7a1b06a12148a42ae",
            "85f4fd5bb26146d0878afdc4524e306a",
            "2370d57b6bbc4deaa82391d9dfe31dfc",
            "61c8764486264e2cabb6ecba958c626f",
            "cbb3682b01c5453ba71e26432c9be45d",
            "cd71c79abeeb4283908e8c91b25d4a1f",
            "952eaa1b18d64470a7c630bfbb45ea56",
            "48785095973d4aba83175623b6047f78",
            "b41ae82df86443c29fef662d50e80380",
            "e70bb407e1e746cc85aefef5d6364c8a",
            "bb3a89169710443587210bb35735feed",
            "f5336974ec0549308fb969320d974446",
            "6abd0b001b154801acd45dfc176f1dc8",
            "d9021f6230914f25bf6d9482dd2039c3",
            "e024c3b3b22f499d8534e005a16e79ee",
            "b1ea6fdfd51940b78d4622ee18817caf",
            "fbcbd19db4fb48638b0034885b6a82e0",
            "e2a148ad1a6347bb869fd5609e786798",
            "90d9e9f252f24b37a4358b5010b06484",
            "73c9cd63aedc47b7b7b1adee99f50fa8",
            "6fabf6d0a411498e945143daafca8433",
            "60ee12069d8344049b65e165da8d779d",
            "a41dc5e7c0084da1822b011f4c982e01",
            "3dd81d60087c46bea6ae1186c7e3618a",
            "2ed09bb92cb44bb08f0806500c90ad17",
            "31d7f8a27d9542efa22b150a93f193e7",
            "e81588b8b7ea49a7b5212e3cd030ee2d",
            "554955c6203e44b9b93e5d574f9f2dca",
            "d04817ae5aff47bab32d3efb58598066",
            "57d91525a8b949e6b37d742cc99aa595",
            "ea8f1f5823a84fb09518c1bc0cc4ad35",
            "10f41b9861d3412888ad840a0152aa47",
            "599be7fe0882427ca99de945daa5ace9",
            "c417cf1435aa40b5b202b95148672e60",
            "a7156d51a54440a08ae7317f2e5a45f1",
            "163492a26774465a8fda6bd9aa87120b",
            "065ae344857849b9b2eb77a954e8037a",
            "0bc4283d1fd0419e983dc4244b8e55ec",
            "af3757b259b24af491cef5a376726c98",
            "e656d2cd7d5b44849e5fd4b97f57d7b9",
            "81556ebd97124bd6914267004cfe0145"
          ]
        },
        "id": "OIImT6qoOcrG",
        "outputId": "e7bbe7fd-0db8-4d4e-ae95-bdb9cccc7776"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating bm25 retriever:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a401930b827456a8ad83a15b26fd591"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "bm25 retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.880\n",
            "  Recall@1: 0.830\n",
            "  Recall@5: 0.950\n",
            "  Precision@1: 0.830\n",
            "  Precision@5: 0.216\n",
            "  latency_mean_s: 0.026\n",
            "  latency_median_s: 0.018\n",
            "  Total misses: 5 / 100. Showing up to 3 misses:\n",
            "   Query: کووڈ-19 ویکسین کا بنیادی مقصد کیا ہے؟\n",
            "    Positives: ['p0007']\n",
            "    Retrieved top ids: ['p0028', 'p0050', 'p0051', 'p0027', 'p0039']\n",
            "   Query: وینٹیلیشن وبا کے دوران کیوں اہم ہے؟\n",
            "    Positives: ['p0020']\n",
            "    Retrieved top ids: ['p0017', 'p0060', 'p0031', 'p0048', 'p0027']\n",
            "   Query: ویکسین سائیڈ ایفیکٹس کی نگرانی کیسے کی جاتی ہے؟\n",
            "    Positives: ['p0039']\n",
            "    Retrieved top ids: ['p0058', 'p0040', 'p0032', 'p0051', 'p0011']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating dense retriever:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "669e6f361a494787a4fd4fde7937cc3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "dense retriever (Fine-tuned) evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.839\n",
            "  Recall@1: 0.740\n",
            "  Recall@5: 0.960\n",
            "  Precision@1: 0.740\n",
            "  Precision@5: 0.216\n",
            "  latency_mean_s: 0.011\n",
            "  latency_median_s: 0.010\n",
            "  Total misses: 4 / 100. Showing up to 3 misses:\n",
            "   Query: ویکسین کی افادیت وقت کے ساتھ کیوں کم ہو سکتی ہے؟\n",
            "    Positives: ['p0032']\n",
            "    Retrieved top ids: ['p0008', 'p0035', 'p0025', 'p0045', 'p0005']\n",
            "   Query: کیا ویکسینیشن کے بعد بھی وائرل پھیلاؤ ممکن ہے؟\n",
            "    Positives: ['p0050', 'p0032']\n",
            "    Retrieved top ids: ['p0008', 'p0052', 'p0059', 'p0043', 'p0007']\n",
            "   Query: ویکسین کی افادیت اور ضمنی اثرات کے بارے میں عوام کو کیا بتائیں؟\n",
            "    Positives: ['p0058', 'p0039']\n",
            "    Retrieved top ids: ['p0051', 'p0056', 'p0026', 'p0034', 'p0008']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating hybrid_interleave retriever:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b41ae82df86443c29fef662d50e80380"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hybrid_interleave retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.839\n",
            "  Recall@1: 0.740\n",
            "  Recall@5: 0.960\n",
            "  Precision@1: 0.740\n",
            "  Precision@5: 0.216\n",
            "  latency_mean_s: 0.011\n",
            "  latency_median_s: 0.010\n",
            "  Total misses: 4 / 100. Showing up to 3 misses:\n",
            "   Query: ویکسین کی افادیت وقت کے ساتھ کیوں کم ہو سکتی ہے؟\n",
            "    Positives: ['p0032']\n",
            "    Retrieved top ids: ['p0008', 'p0035', 'p0025', 'p0045', 'p0005']\n",
            "   Query: کیا ویکسینیشن کے بعد بھی وائرل پھیلاؤ ممکن ہے؟\n",
            "    Positives: ['p0050', 'p0032']\n",
            "    Retrieved top ids: ['p0008', 'p0052', 'p0059', 'p0043', 'p0007']\n",
            "   Query: ویکسین کی افادیت اور ضمنی اثرات کے بارے میں عوام کو کیا بتائیں؟\n",
            "    Positives: ['p0058', 'p0039']\n",
            "    Retrieved top ids: ['p0051', 'p0056', 'p0026', 'p0034', 'p0008']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating hybrid_score retriever:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73c9cd63aedc47b7b7b1adee99f50fa8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hybrid_score retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.932\n",
            "  Recall@1: 0.880\n",
            "  Recall@5: 0.990\n",
            "  Precision@1: 0.880\n",
            "  Precision@5: 0.228\n",
            "  latency_mean_s: 0.011\n",
            "  latency_median_s: 0.010\n",
            "  Total misses: 1 / 100. Showing up to 3 misses:\n",
            "   Query: کووڈ-19 کے خلاف طویل مدتی تیاری میں کون سی چیزیں شامل ہونی چاہئیں؟\n",
            "    Positives: ['p0060', 'p0052']\n",
            "    Retrieved top ids: ['p0021', 'p0044', 'p0025', 'p0059', 'p0046']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating hybrid_rrf retriever:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea8f1f5823a84fb09518c1bc0cc4ad35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hybrid_rrf retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.901\n",
            "  Recall@1: 0.830\n",
            "  Recall@5: 1.000\n",
            "  Precision@1: 0.830\n",
            "  Precision@5: 0.232\n",
            "  latency_mean_s: 0.011\n",
            "  latency_median_s: 0.010\n",
            "  Total misses: 0 / 100. Showing up to 3 misses:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hybrid Retriever model has amazing accuracy and thus we are good to move on to the generator model and the integration of both into a RAG model."
      ],
      "metadata": {
        "id": "5AOFIcOuOyLf"
      }
    }
  ]
}
